"""Sim_clean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aWq9g-VoaNYX8PllsufC-FS1rtrsDCqT
"""


import Bio
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from Bio.Seq import MutableSeq
from Bio import SeqIO
from Bio.SeqIO import write
import subprocess
from subprocess import run
import numpy as np
from numpy import random
import random
from scipy.stats import skewnorm #for fitting to curve
from matplotlib import pyplot as plt #plotting
import time
import multiprocessing
from multiprocessing import Pool, cpu_count, Manager
import cProfile
import pstats
import os

"""Testing Sequences:"""

ecoli_fasta = os.path.join('/home/jyeh/summer2024', 'ecoli.fasta')
#putting the ecoli genome into a testable variable
for seq_record in SeqIO.parse(ecoli_fasta, "fasta"):
    ecoli_seq = seq_record.seq
    #is written 5' to 3'

# print(ecoli_seq[:10])
# print(ecoli_seq[-10:])

test_seq = ecoli_seq[:20000]

plasmid_fasta = os.path.join('/home/jyeh/summer2024', 'gbmam8.fasta')
for seq_record in SeqIO.parse(plasmid_fasta, "fasta"):
    plasmid_seq = seq_record.seq
    
def get_sequence(file_path):
    for seq_record in SeqIO.parse(file_path, "fasta"):
        return seq_record.seq

"""#Tn5 Transposase Tagementation"""

#breakup sequences into multiple random segments
def splitter(sequence, verbose = False):
    res = []
    splits = []
    length  = len(sequence)
    split = 0

    a = 3  # Skewness parameter
    loc = 500  # Location parameter (mean)
    scale = 200  # Scale parameter (standard deviation approximation)
    min_length = 150 #minimum transposon length
    max_length = 10000 # max length of transposon

    while split + min_length < length: #so last split does not go out of range of sequence (cannot be split + max_length or the last fragment will  be massive)
        begin = split #begining index of each fragment

        split_len = int(skewnorm.rvs(a, loc=loc, scale=scale)) #randomize / fit length of fragments to normal curve, includes parameters for min / max lengths
        
        if verbose == True:
          splits.append(begin)

        if split + split_len >= length - min_length:
            split_len = length - split # If the next split would go beyond the sequence length, adjust the split length

        # Adjust split location to account for insertion bias
        else:
          while True:
              if split + split_len >= length or split == 0:  # Avoid out-of-range errors
                  break

              chance = random.randint(1, 100)
              if sequence[split + split_len] == 'G' and chance < 50:
                  break
              elif sequence[split + split_len + 8] == 'C' and chance < 50:
                  break
              else:
                  if chance < 75:
                      split_len += 1
                  else:
                      break

        #keep split length constraints (min and max lengths)
        if split_len < min_length:
            split_len = min_length
        elif split_len > max_length:
            split_len = max_length

        split += split_len
        res.append(sequence[begin:split]) #append spliced fragment
        
    if verbose == True:
        return splits

    return res

#From splitter, get overlapping 9 bp for same side
def overlapper(seq_arr):
    res = [seq_arr[-1][-9:] + seq_arr[0]] #start with list of first fragment with last nine base pairs of last fragment added on (bacterial DNA is circular, no beginning and no end)
    for i in range(1, len(seq_arr)): #for number of fragments
        res.append(seq_arr[i-1][-9:] + seq_arr[i]) #add the last 9 bp of previous fragment to next fragment
    return res

#print(overlapper(splitter(sequence)))



#puts splitter & overlapper together
def get_fragments(sequence):
    return (overlapper(splitter(sequence)))


"""Splitting statistics:"""

def frag_stats(fragments):
  longest_frag = 0
  shortest_frag = np.inf
  frag_first_bases = [0, 0, 0, 0] #in order ACTG
  frag_fourth_bases = [0, 0, 0, 0]
  frag_ninth_bases = [0, 0, 0, 0]

  for frag in fragments:
    if frag[9] == 'A':
      frag_first_bases[0] += 1
    elif frag[9] == 'C':
      frag_first_bases[1] += 1
    elif frag[9] == 'T':
      frag_first_bases[2] += 1
    elif frag[9] == 'G':
      frag_first_bases[3] += 1

    if frag[11] == 'A':
      frag_fourth_bases[0] += 1
    elif frag[11] == 'C':
      frag_fourth_bases[1] += 1
    elif frag[11] == 'T':
      frag_fourth_bases[2] += 1
    elif frag[11] == 'G':
      frag_fourth_bases[3] += 1

    if frag[17] == 'A':
      frag_ninth_bases[0] += 1
    elif frag[17] == 'C':
      frag_ninth_bases[1] += 1
    elif frag[17] == 'T':
      frag_ninth_bases[2] += 1
    elif frag[17] == 'G':
      frag_ninth_bases[3] += 1

  bases = ['A', 'C', 'T', 'G']

  plt.figure(figsize=(10, 4))
  plt.bar(bases, frag_first_bases)
  plt.title('Tn5 Insertion Bias First Base')
  plt.xlabel('First Duplication Base')
  plt.ylabel('Number of Occurrences')
  plt.show()

  plt.figure(figsize=(10, 4))
  plt.bar(bases, frag_fourth_bases)
  plt.title('Tn5 Insertion Bias Fourth Base')
  plt.xlabel('Fourth Duplication Base')
  plt.ylabel('Number of Occurrences')
  plt.show()

  plt.figure(figsize=(10, 4))
  plt.bar(bases, frag_ninth_bases)
  plt.title('Tn5 Insertion Bias Ninth Base')
  plt.xlabel('Ninth Duplication Base')
  plt.ylabel('Number of Occurrences')
  plt.show()

  for frag in fragments:
    if len(frag) > longest_frag:
      longest_frag = len(frag)  #iterate through all fragments till the longest is found
    if len(frag) < shortest_frag:
      shortest_frag = len(frag) #iterate through all fragments till the shortest is found

  print('shortest fragment without adapters:', shortest_frag)
  print('longest fragment without adapters:', longest_frag)

  #number of fragments
  print ('number of fragments before PCR from 1 strand:', len(fragments))
  #number of bases
  print('bases in genome:', len(ecoli_seq))


#plot fragment lengths
def plot_frag_lengths(fragments):
  frag_len_list = []

  longest_frag = 0
  shortest_frag = np.inf
  for frag in fragments:
    if len(frag) > longest_frag:
      longest_frag = len(frag)  #iterate through all fragments till the longest is found
    if len(frag) < shortest_frag:
      shortest_frag = len(frag) #iterate through all fragments till the shortest is found

  for i in range(longest_frag+1):
    frag_len_list.append(0)

  for frag in fragments:
    frag_len_list[len(frag)] += 1

  #print(frag_len_list)

  plt.bar(range(len(frag_len_list)), frag_len_list)
  plt.title('Fragment Lengths Distribution')
  plt.xlabel('Fragment Length without adapters (base pairs)')
  plt.ylabel('Frequency before PCR (number of fragments)')
  plt.grid(True)
  plt.show()


"""adding adapters:"""

def add_adapters(fragments, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'], verbose = False): #adapters list in [p5, p7] listed from 5' to 3' from illumina
  #add p5 and p7 adapters to end 5' end of each fragment, and the complimentary of p5 or p7 to the 3' end of each fragment. Also keep track of readable fragments (different adapters on each end)
  coded_fragments = [0]*len(fragments)
  num_readable = 0 #initialize number of readable fragments as 0

  for i in range(len(fragments)):
    forward_adapter = Seq(str(random.choice(adapters))) # Randomly select the next adapter for the forward end

    # Randomly select the next adapter for the reverse end; since this is the 3' end, this will be the compliment of the adapter added.
    reverse_adapter = Seq(str(random.choice(adapters))).reverse_complement()

    # Form the fragment with adapters
    coded_fragments[i] = forward_adapter + fragments[i] + reverse_adapter

    #fragment is only readable if fwd & rvs adapter are different; checks if they are complements of eachother
    if coded_fragments[i][:len(adapters[0])] == Seq(adapters[0]):
      if coded_fragments[i][-len(adapters[1]):] == Seq(adapters[1]).reverse_complement(): #if the other end is complement of other adapter
        num_readable+=1 #keep track of how many ligated fragments are readable

    else:
      #means that fragment starts with p7 adapter, will check if other end is compliment of p5 adapter
      if coded_fragments[i][-len(adapters[0]):] == Seq(adapters[0]).reverse_complement():
        num_readable+=1

  if verbose == True:
    print('number of readable fragments from 1 strand:', num_readable)

  return coded_fragments


"""Overall Tn5 Tranposase Tagmentation Function:"""

def use_transposase(genome, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'], verbose = False):
  splits = get_fragments(genome) #split genome into fragments; calls splitter function
  ligated_fragments = add_adapters(splits, adapters, verbose) #add adapters to the fragments; calls add_adapters function
  num_fragments = len(ligated_fragments)

  #create complementary fragments:
  for i in range(num_fragments):
    ligated_fragments.append(ligated_fragments[i].reverse_complement())

  if verbose == True:
    frag_stats(splits)
    plot_frag_lengths(splits)

    #calculating number of readable fragments
    readable = 0
    for frag in ligated_fragments:
      #fragment is only readable if fwd & rvs adapter are different; checks if they are complements of eachother and if readable adds 1 to readable
      if frag[:len(adapters[0])] == Seq(adapters[0]):
        if frag[-len(adapters[1]):] == Seq(adapters[1]).reverse_complement(): #if the other end is complement of other adapter
          readable +=1

      elif frag[:len(adapters[1])] == Seq(adapters[1]):
        #means that fragment starts with p7 adapter, will check if other end is compliment of p5 adapter
        if frag[-len(adapters[0]):] == Seq(adapters[0]).reverse_complement():
          readable +=1

    print('single stranded fragments:', num_fragments)
    print('number of fragments after adding complements:', len(ligated_fragments))
    print('number of readable fragments before pcr from both strands:', readable)

  return ligated_fragments

"""#Using Transposase on multiple genome copies"""
def transposase_task(args):
    genome, adapters = args
    return use_transposase(genome, adapters)

def create_fragments(genome, num_genomes, adapters):
    num_cores = min(num_genomes, cpu_count())
    args = [(genome, adapters)] * num_genomes
    
    with Pool(processes=num_cores) as pool:
        # Pass transposase_task function to map, and range(num_genomes) as the iterable
        fragments = pool.map(transposase_task, args)

    # Flatten the list of lists
    fragments = [fragment for sublist in fragments for fragment in sublist]

    return fragments

"""#plotting for overall fragment library"""

def lib_stats(fragments, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT']):
  longest_frag = 0
  shortest_frag = np.inf
  frag_first_bases = [0, 0, 0, 0] #in order ACTG
  frag_fourth_bases = [0, 0, 0, 0]
  frag_ninth_bases = [0, 0, 0, 0]

  len_p5 = len(adapters[0])
  len_p7 = len(adapters[1])

  for frag in fragments:
    adap_len = 0
    if frag[:len_p5] == Seq(adapters[0]):
      adap_len = len_p5
    elif frag[:len_p7] == Seq(adapters[1]):
      adap_len = len_p7

    if frag[adap_len + 9] == 'A':
      frag_first_bases[0] += 1
    elif frag[adap_len + 9] == 'C':
      frag_first_bases[1] += 1
    elif frag[adap_len + 9] == 'T':
      frag_first_bases[2] += 1
    elif frag[adap_len + 9] == 'G':
      frag_first_bases[3] += 1

    if frag[adap_len + 11] == 'A':
      frag_fourth_bases[0] += 1
    elif frag[adap_len + 11] == 'C':
      frag_fourth_bases[1] += 1
    elif frag[adap_len + 11] == 'T':
      frag_fourth_bases[2] += 1
    elif frag[adap_len + 11] == 'G':
      frag_fourth_bases[3] += 1

    if frag[adap_len + 17] == 'A':
      frag_ninth_bases[0] += 1
    elif frag[adap_len + 17] == 'C':
      frag_ninth_bases[1] += 1
    elif frag[adap_len + 17] == 'T':
      frag_ninth_bases[2] += 1
    elif frag[adap_len + 17] == 'G':
      frag_ninth_bases[3] += 1

  bases = ['A', 'C', 'T', 'G']

  plt.figure(figsize=(10, 4))
  plt.bar(bases, frag_first_bases)
  plt.title('Tn5 Insertion Bias First Base')
  plt.xlabel('First Duplication Base')
  plt.ylabel('Number of Occurrences')
  plt.savefig('/home/jyeh/summer2024/first_base.png')

  plt.figure(figsize=(10, 4))
  plt.bar(bases, frag_fourth_bases)
  plt.title('Tn5 Insertion Bias Fourth Base')
  plt.xlabel('Fourth Duplication Base')
  plt.ylabel('Number of Occurrences')
  plt.savefig('/home/jyeh/summer2024/fourth_base.png')

  plt.figure(figsize=(10, 4))
  plt.bar(bases, frag_ninth_bases)
  plt.title('Tn5 Insertion Bias Ninth Base')
  plt.xlabel('Ninth Duplication Base')
  plt.ylabel('Number of Occurrences')
  plt.savefig('/home/jyeh/summer2024/ninth_base.png')

  for frag in fragments:
    if len(frag) > longest_frag:
      longest_frag = len(frag)  #iterate through all fragments till the longest is found
    if len(frag) < shortest_frag:
      shortest_frag = len(frag) #iterate through all fragments till the shortest is found

  print('shortest fragment without adapters:', shortest_frag)
  print('longest fragment without adapters:', longest_frag)

  #number of fragments
  print ('number of fragments before PCR from 1 strand:', len(fragments))
  #number of bases
  print('bases in genome:', len(ecoli_seq))


#to save memory and time, only keep readable fragments
def dispose_unreadables(fragments, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT']):
  res = []
  for frag in fragments:
    if frag[:len(adapters[0])] == Seq(adapters[0]) and frag[-len(adapters[1]):] == Seq(adapters[1]).reverse_complement():
      res.append(frag)
    elif frag[:len(adapters[1])] == Seq(adapters[1]) and frag[-len(adapters[0]):] == Seq(adapters[0]).reverse_complement():
      res.append(frag)
  return res


"""#PCR"""
def pcr_worker(frag, cycles, plateau):
    gc_count = sum(1 for base in frag if base in 'GC')
    gc_percent = (gc_count / len(frag)) * 100  # Calculate GC content as a percentage

    if plateau[0] <= gc_percent <= plateau[1]:
        amp_factor = 1.0  # 100% relative abundance within the plateau range
    elif gc_percent < plateau[0]:
        amp_factor = 0.05 + (0.95 / plateau[0]) * gc_percent  # Linear increase from 5% to 100%
    elif gc_percent <= plateau[1] + 20:
        amp_factor = 1.0 - (0.2 / 20) * (gc_percent - plateau[1])  # for using Phusion HF & 2M Betaine ->Linear decrease from 100% to 80% relative abundance
#                                                                     for using normal solution, this should be linear decrease from 100 to 1% or 0.99/20
    else:
        amp_factor = 0.80  # 80% relative abundance above the extended plateau range; if using normal solution, this should be 1% or 0.01

    copies = 1
    for _ in range(cycles):
        copies *= (2 ** amp_factor)  # Amplification factor of 2x for each cycle
    copies = int(copies)  # Update the number of copies, flooring each value to an integer (cannot have partial copies)
    
    # #no bias pcr
    # copies = 1
    # for _ in range(cycles):
    #   copies *= 2
    # copies = int(copies)

    return frag, [copies, gc_percent]


def pcrer(fragments, cycles, plateau = [10, 50], verbose = False): #plateau is the range which relative abundance after PCR is 100%, can be changed depending on PCR procedure
  with multiprocessing.Pool() as pool:
        results = pool.starmap(pcr_worker, [(frag, cycles, plateau) for frag in fragments])

  library = dict(results)

  if verbose:
      # Gather data for plotting
      gc_content = [library[frag][1] for frag in library]
      copy_numbers = [library[frag][0] for frag in library]

      # Plotting
      plt.figure(figsize=(10, 6))
      plt.scatter(gc_content, copy_numbers, alpha=0.6)
      plt.xlabel('GC Content (%)')
      plt.ylabel('Number of Copies')
      plt.title('PCR Amplification with GC Content Bias')
      plt.grid(True)
      plt.show()

  return library


"""#Formatting the library"""

def shuffler(fragments, verbose = False): #need to add PCR biases later
  #randomizes order of fragments as they would be in a solution
  keys = list(fragments.keys())
  random.shuffle(keys)
  shuffled_dict = {key: fragments[key] for key in keys}
  fragments = shuffled_dict

  num_frags = sum(values[0] for values in fragments.values())

  if verbose == True:
    print('number of readable fragments after PCR:', num_frags)

  return fragments


"""#Reading Fragments"""

#output reads of the fragments
#read from both sides a specified number of bases (forward & reverse reads)
#be able to relate both reads to the correct sequence
#unknowns: number of each fragment, order of fragments
#each flowcell takes 25000000 samples each time


def get_batch_of_fragments(fragments, num_flowcells, flowcell_size, verbose= False):
    samples = {}
    # Extract the fragments (keys) and their weights (number of copies)
    fragment_sequences = list(fragments.keys())
    weights = [values[0] for values in fragments.values()]

    # Create a mapping from fragment to index for quick lookups
    fragment_to_index = {fragment: i for i, fragment in enumerate(fragment_sequences)}

    batch_size = 1000000 #batch size or fragments chosen each time
    num_samples = 0
    while num_samples < num_flowcells * flowcell_size:
        # Use random.choices to pick one fragment based on weights
        selected_fragments = random.choices(fragment_sequences, weights=weights, k=batch_size) #multiple samples at a time to reduce runtime (weights are not exactly correct but o welp)

        for j in range(batch_size):
          if num_samples >= num_flowcells * flowcell_size:
              break
          # Add the selected fragment to the sample dictionary
          num_samples += 1
          if selected_fragments[j] not in samples: #if not in dictionary
              samples[selected_fragments[j]] = 1 #put in dictionary
          else:
              samples[selected_fragments[j]] += 1 #increase number of copies if already in dictionary

          # Update the fragments dictionary
          # Decrease the number of copies for the selected fragment
          selected_fragment_index = fragment_to_index[selected_fragments[j]]
          weights[selected_fragment_index] -= 1
          # If no copies are left, fragment will never be chosen because weight is zero
          # Ensure the weight does not go below zero
          if weights[selected_fragment_index] < 0:
              weights[selected_fragment_index] = 0

    if verbose == True:
      print('number of unique fragments in sample:', len(samples))
      print('number of readable fragments in sample:', sum(values for values in samples.values()))

    return samples
  
def trim_adapters(samples, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'], verbose = False):
  res = {}
  len_p5 = len(adapters[0])
  len_p7 = len(adapters[1])

  for frag in samples:
    if frag[:len_p5] == Seq(adapters[0]):
      res[frag[len_p5:-len_p7]] = samples[frag]
    elif frag[:len_p7] == Seq(adapters[1]):
      res[frag[len_p7:-len_p5]] = samples[frag]
      
  if verbose == True:
    print('number of trimmed reads:', sum(values for values in res.values()))
    print('number of unique trimmed reads:', len(res))

  return res


def produce_errors(args):
    fragment, direction = args
    new_fragment = list(fragment)

    for i in range(len(new_fragment)):
        if len(new_fragment) <= i:  # Check if index is out of bounds
            break

        base = new_fragment[i]

        # will there be an error? Chance of error depends on the base
        error = False
        error_chance = random.uniform(0.00, 100.00)
        if (base == 'A' and error_chance <= 0.08 * direction) or (base == 'C' and error_chance <= 0.08 * direction):
            error = True
        elif base == 'G' and error_chance <= 0.10 * direction:
            error = True
        elif base == 'T' and error_chance <= 0.16 * direction:
            error = True

        if error:
            error_type_chance = random.uniform(0.00, 100.00)
            if error_type_chance <= 0.12:
                del new_fragment[i]  # deletion
            elif error_type_chance <= 0.20:
                new_fragment.insert(i, random.choice(['A', 'C', 'G', 'T']))  # insertion
            else:
                #choosing base to be substituted
                bases = ['A', 'C', 'G', 'T']
                base_index = bases.index(base)
                base_chance = random.randint(1, 4)
                if base_chance < 3:
                    sub_base = bases[base_index - 2]
                elif base_chance == 3:
                    sub_base = bases[base_index - 1]
                else:
                    sub_base = bases[base_index - 3]

                new_fragment[i] = sub_base

    return Seq(''.join(new_fragment))


def fwd_reader(sample_fragments, read_length): #returns list of forward reads
    num_processes = multiprocessing.cpu_count()  # Get the number of CPU cores

    with multiprocessing.Pool(processes=num_processes) as pool:
        args_list = [(fragment[:read_length], 1) if len(fragment) > read_length else (fragment, 1) for fragment, copies in sample_fragments.items() for _ in range(copies)]
        results = pool.map(produce_errors, args_list)

    return results

def rvs_reader(sample_fragments, read_length): #returns list of reverse reads
    num_processes = multiprocessing.cpu_count()  # Get the number of CPU cores

    with multiprocessing.Pool(processes=num_processes) as pool:
        args_list = [(fragment[-read_length:], 2) if len(fragment) > read_length else (fragment, 2) for fragment, copies in sample_fragments.items() for _ in range(copies)]
        results = pool.map(produce_errors, args_list)

    return results

def read_combiner(fwd_reads, rvs_reads, verbose = False): #usually, this is where bridge PCR would occur to create the fluorescent intensity for reads.
  num_reads = len(fwd_reads)
  res = []

  for i in range(num_reads):
    res.append([fwd_reads[i], rvs_reads[i]])

  if verbose == True:
    print('number of output reads:', num_reads)

  random.shuffle(res) #randomize order of reads

  return res

def reader(fragments, read_length, num_flowcells, flowcell_size, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'], verbose = False):
  sample = get_batch_of_fragments(fragments, num_flowcells, flowcell_size) #get a batch of fragments
  trimmed = trim_adapters(sample, adapters, verbose) #trim adapters
  reads = read_combiner(fwd_reader(trimmed, read_length), rvs_reader(trimmed, read_length), verbose) #combines all reader helper fuctions
  del sample
  return reads


'''Turning reads into fasta files, (a reverse read file and a fwd read file), for bowtie2 alignment'''
def save_reads_to_fasta(reads, fwd_filename, rvs_filename):
    # Save forward and reverse reads to separate FASTA files.

    # Args:# reads (list): List of read pairs in the form [fwd_read, rvs_read], fwd_filename (str): Output filename for forward reads, rvs_filename (str): Output filename for reverse reads.
    with open(fwd_filename, 'w') as fwd_file, open(rvs_filename, 'w') as rvs_file:
        for i, (fwd_read, rvs_read) in enumerate(reads):
            fwd_file.write(f'>fwd_read_{i}\n{fwd_read}\n')
            rvs_file.write(f'>rvs_read_{i}\n{rvs_read}\n')


def main():

  """#Implementation library creation + reading"""
  start = time.time()
  #parameters
  verbose = False      #print figures and stats or not
  genome = get_sequence('/home/jyeh/summer2024/ecoli.fasta')  #input initial genome
#   genome = plasmid_seq
  num_genomes = 200   #change this number for how many genomes there are; 100 is best for our application
  adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'] #adapters used for transposase tagmentation
  plateau = [11, 84]  #pcr plateau range depending on protocol
  cycles_of_pcr = 16   #number of cycles of pcr
  num_flowcells = 1   #number of flowcells used for reading (will multiply number by 96 for batch size)
  read_length = 250   #length of reads determined by machine
  flowcell_size = 2000000  #number of samples per flowcell; miseq has 25 million reads per flowcell, micro has 8 million reads per flowcell, nano has 2 million reads per flowcell
  fwd_filename = '/home/jyeh/summer2024/test_fwd_reads.fasta'
  rvs_filename = '/home/jyeh/summer2024/test_rvs_reads.fasta'

  #creating fragment library
  fragments = []
  time1 = time.time()
  fragments = create_fragments(genome, num_genomes, adapters)
  time2 = time.time()
  print('fragmentation runtime:', time2-time1, 'seconds')
  print('number of fragments after transposase:', len(fragments))

  #stats for all the fragments after using multiple genome copies
  if verbose == True:
    lib_stats(fragments, adapters)
    plot_frag_lengths(fragments)

  #pcr and library prep
  time1 = time.time()
  fragments = dispose_unreadables(fragments, adapters) #gets rid of unreadable fragments in fragments
  print('number of readable fragments before pcr:', len(fragments))
  copies = pcrer(fragments, cycles_of_pcr, plateau, verbose)
  print('number of readable fragments after pcr:', sum(values[0] for values in copies.values()))
  del fragments
  library = shuffler(copies, verbose)
  time2 = time.time()
  print('pcr and library prep runtime:', time2-time1, 'seconds')

  #reading fragments
  read_time1 = time.time()
  read_library = reader(library, read_length, num_flowcells, flowcell_size, adapters, verbose)
  read_time2 = time.time()
  print('reading runtime:', read_time2-read_time1, 'seconds')
  del library
  
  file_time1 = time.time()
  save_reads_to_fasta(read_library, fwd_filename, rvs_filename)
  file_time2 = time.time()
  print('file saving runtime:', file_time2-file_time1, 'seconds')

  end = time.time()
  #print(read_library)

  print('length of read library:', len(read_library))
  print(read_library[0])
  #print(read_library)
  print('total time:', end-start, 'seconds')


  """Testing Tn5 function"""
  # splits = get_fragments(ecoli_seq) #calling get_fragments function
  # print(splits)
  # print(splits[-1])

  # frag_stats(splits) #4641652 bases in the genome
  # plot_frag_lengths(splits) #plot split lengths distribution

  # fragments = add_adapters(splits, verbose = True)
  # print(fragments)
  # print(fragments[-1])

  # fragments = use_transposase(ecoli_seq, verbose = True)
  # print(fragments)

  """Testing PCR & Library prep"""
  # fragments = use_transposase(ecoli_seq, verbose = False)
  # library = pcrer(fragments, 35, plateau = [10, 50], verbose = True)
  # print(library)
  # lib_stats(library)


  # fragments = use_transposase(ecoli_seq, verbose = False)
  # copies = pcrer(fragments, 35, plateau = [10, 50], verbose = False)
  # library = shuffler(copies, verbose = True)
  # print(library)

  """Testing Reading"""
  # time1 = time.time()
  # fragments = create_fragments(ecoli_seq, 10, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'])
  # copies = pcrer(fragments, 35, plateau = [10, 50], verbose = False)
  # library = shuffler(copies, verbose = False)
  # batch = get_batch_of_fragments(library, 1, 10000000, verbose = True)
  # time2 = time.time()
  # # print(batch)
  # print('fragmentation + batch runtime:', time2-time1, 'seconds')

  # time3 = time.time()
  # trimmed_reads = trim_adapters(batch, verbose = True)
  # time4 = time.time()
  # print('trimming runtime:', time4-time3, 'seconds')
  
  # time3 = time.time()
  # fwd_reads = fwd_reader(trimmed_reads, 250)
  # rvs_reads = rvs_reader(trimmed_reads, 250)
  # print('number of fwd reads:', len(fwd_reads))
  # print('number of rvs reads:', len(rvs_reads))
  # time4 = time.time()
  # print('reading runtime:', time4-time3, 'seconds')
  # print('fwd & rvs reads:', fwd_reads[0], rvs_reads[0], fwd_reads[-1], rvs_reads[-1])
  

  # reads = read_combiner(fwd_reads, rvs_reads, verbose = True)
  # print('reads combined:', reads[0], reads[-1])

  """testing reading errors"""
  # time1 = time.time()
  # test_batch = {Seq('ACTGACTGACTGACTGACTGACTGACTGACTGAC'): 10000000}
  # #to test error production
  # #print(test_batch)
  # fwd_read_test = fwd_reader(test_batch, 30)
  # print(fwd_read_test[0])
  # time2 = time.time()
  # print('runtime:', time2-time1, 'seconds')

  """#Testing Full Pipeline"""
  # time1 = time.time()
  # fragments = use_transposase(ecoli_seq, verbose = False)
  # copies = pcrer(fragments, 35, plateau = [10, 50], verbose = False)
  # del fragments
  # library = readability_and_shuffle(copies, verbose = False)
  # del copies
  # reads = reader(library, 250, 1, 25000000, verbose = True)
  # del library
  # time2 = time.time()
  # print(reads[:10])
  # print('number of reads:', len(reads))
  # print('total runtime:', time2-time1, 'seconds')

  """optimizing parallelization of fragmentation"""
  #get_fragments uses parallelization to create fragments from 500 copies of genome
  # time1 = time.time()
  # fragments = create_fragments(ecoli_seq, 10, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'])
  # time2 = time.time()
  # print('total runtime parallel:', time2-time1, 'seconds')

  # time3 = time.time()
  # for i in range(500):
  #   fragments = use_transposase(ecoli_seq, verbose = False)
  # time4 = time.time()
  # print('total runtime linear:', time4-time3, 'seconds')

  '''#parallelization of read batch collecting'''
  # time1 = time.time()
  # fragments = create_fragments(ecoli_seq, 10, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'])
  # copies = pcrer(fragments, 35, plateau = [10, 50], verbose = False)
  # del fragments
  # library = readability_and_shuffle(copies, verbose = False)
  # del copies
  # time2 = time.time()
  # print('pcr and library prep runtime:', time2-time1, 'seconds')
  # # parallel_batch = get_batch_of_fragments_parallel(library, 1, 4000000, verbose = True)
  # time3 = time.time()
  # print('parallel runtime:', time3-time2, 'seconds')
  # original_batch = get_batch_of_fragments(library, 1, 10000000, verbose = True)
  # time4 = time.time()
  # print('linear runtime:', time4-time3, 'seconds')
  
  '''coverage stats'''
  # splitter(test_seq, verbose = True)
  # num_seq = 100
  # split_list = []
  # for i in range(num_seq):
  #   split_list.append(splitter(test_seq, verbose = True))
    
  # plt.hist(split_list, bins = 100)
  # plt.title('Fragment Coverage Distribution')
  # plt.xlabel('Number of Fragments')
  # plt.ylabel('Number of Occurrences')
  # plt.grid(True)
  # plt.savefig('/u/c/yehjose1/summer2024/split_coverage.png')
  
  '''batching coverage testing'''
  # start = time.time()
  # fragments = create_fragments(plasmid_seq, 10000, adapters = ['AATGATACGGCGACCACCGA',  'CAAGCAGAAGACGGCATACGAGAT'])
  # print('time to create fragments:', time.time()-start)
  # copies = pcrer(fragments, 35, plateau = [10, 50], verbose = False)
  # del fragments
  # print('time to pcr:', time.time()-start)
  # library = shuffler(copies, verbose = False)
  # del copies
  # batch = get_batch_of_fragments(library, 1, 100000, verbose = True)
  # print('time to batch:', time.time()-start)
  # sample = trim_adapters(batch)
  # print('time to trim:', time.time()-start)
  
  # records = []
  # for i, (fragment, copies) in enumerate(sample.items(), start = 1):
  #   for j in range(copies):
  #     record = SeqRecord(fragment, id = f'read_{i}_{j+1}', description = f"Fragment {i} copy {j+1}")
  #     records.append(record)
      
  # with open('/u/c/yehjose1/summer2024/batch_coverage_test.fasta', 'w') as output_handle:
  #   SeqIO.write(records, output_handle, 'fasta')
    
  
  # print('fasta file created in:', time.time()-start)
  


if __name__ == '__main__':
  print('cpus:', multiprocessing.cpu_count())
  main()


'''to run bowtie2'''
#1, bowtie2-build /u/c/yehjose1/summer2024/ecoli.fasta index_prefix  #to build index reference genome
#2, bowtie2 -f -x /u/c/yehjose1/summer2024/index_prefix -1 /u/c/yehjose1/summer2024/ecoli_fwd_reads.fasta -2 /u/c/yehjose1/summer2024/ecoli_rvs_reads.fasta -S /u/c/yehjose1/summer2024/ecoli_aligned.sam
#   or for multithreading: bowtie2 -f -p 16 -x /u/c/yehjose1/summer2024/index_prefix -1 /u/c/yehjose1/summer2024/ecoli_fwd_reads.fasta -2 /u/c/yehjose1/summer2024/ecoli_rvs_reads.fasta -S /u/c/yehjose1/summer2024/ecoli_aligned.sam
#   where the # of cpus is after -p

# bowtie2 -f -x /u/c/yehjose1/summer2024/short_index -1 /u/c/yehjose1/summer2024/short_fwd_reads.fasta -2 /u/c/yehjose1/summer2024/short_rvs_reads.fasta -S /u/c/yehjose1/summer2024/short_aligned.sam -p 16
# 
# bowtie2 -f -x /u/c/yehjose1/summer2024/plasmid_prefix -1 /u/c/yehjose1/summer2024/plasmid_fwd_reads.fasta -2 /u/c/yehjose1/summer2024/plasmid_rvs_reads.fasta -S /u/c/yehjose1/summer2024/plasmid_aligned.sam -p 16
# 
# bowtie2 -f -p 16 -x /u/c/yehjose1/summer2024/index_prefix -1 /u/c/yehjose1/summer2024/optimized_pcr_fwd_reads_6cycles.fasta -2 /u/c/yehjose1/summer2024/optimized_pcr_rvs_reads_6cycles.fasta -S /u/c/yehjose1/summer2024/optimized_pcr_aligned_6cycles.sam
#  bowtie2 -f -p 16 -x /u/c/yehjose1/summer2024/index_prefix -1 /u/c/yehjose1/summer2024/optimized_pcr_fwd_reads_16cycles.fasta -2 /u/c/yehjose1/summer2024/optimized_pcr_rvs_reads_16cycles.fasta -S /u/c/yehjose1/summer2024/optimized_pcr_aligned_16cycles.sam
# bowtie2 -f -p 16 -x /u/c/yehjose1/summer2024/index_prefix -1 /u/c/yehjose1/summer2024/optimized_pcr_fwd_reads_35cycles.fasta -2 /u/c/yehjose1/summer2024/optimized_pcr_rvs_reads_35cycles.fasta -S /u/c/yehjose1/summer2024/optimized_pcr_aligned_35cycles.sam

# bowtie2 -f -p 12 -x /home/jyeh/summer2024/index_prefix -1 /home/jyeh/summer2024/no_pcr_fwd_reads.fasta -2 /home/jyeh/summer2024/no_pcr_rvs_reads.fasta -S /home/jyeh/summer2024/no_pcr_.sam

# bowtie2 -f -p 12 -x /home/jyeh/summer2024/index_prefix -U /home/jyeh/summer2024/spades_output_16cycles/contigs.fasta -S /home/jyeh/summer2024/coverage_spades_contigs_16_cycles.sam


'''to run spades'''
# spades.py -1 /u/c/yehjose1/summer2024/optimized_pcr_fwd_reads_16cycles.fastq -2 /u/c/yehjose1/summer2024/optimized_pcr_rvs_reads_16cycles.fastq --phred-offset 64 -o /u/c/yehjose1/summer2024/spades_output -t 16 -m 600
# for illumina reads, --phred offset is 64; for sanger reads, phred offset is 33
# -t = number of threads or cpus
# need to unlimit # of open files for spades to run properly: ulimit -n 1000000; raise # if needed
# spades.py -s /u/c/yehjose1/summer2024/SRA_reads/test_SRA_from_fasta.fastq -o /u/c/yehjose1/summer2024/SRA_reads/spades_test_output -t 80 --phred-offset 64 -m 600

# spades.py --trusted-contigs /home/jyeh/summer2024/assembler_dict_scaffold_5_genomes.fasta -1 /home/jyeh/summer2024/genomes_5_fwd_reads.fastq -2 /home/jyeh/summer2024/genomes_5_rvs_reads.fastq -o spades_5_genome_contigs -t 12 --phred-offset 64
# spades.py --trusted-contigs /home/jyeh/summer2024/assembler_dict_scaffold_1_genomes.fasta -1 /home/jyeh/summer2024/genomes_1_fwd_reads.fastq -2 /home/jyeh/summer2024/genomes_1_rvs_reads.fastq -o spades_1_genome_contigs -t 12 --phred-offset 64
# spades.py --trusted-contigs /home/jyeh/summer2024/assembler_dict_scaffold_16_pcr.fasta -1 /home/jyeh/summer2024/optimized_pcr_fwd_reads_16cycles.fastq -2 /home/jyeh/summer2024/optimized_pcr_rvs_reads_16cycles.fastq -o spades_100_genome_contigs -t 12 --phred-offset 64
# spades.py --trusted-contigs /home/jyeh/summer2024/assembler_dict_scaffold_50_genomes.fasta -1 /home/jyeh/summer2024/genomes_50_fwd_reads.fastq -2 /home/jyeh/summer2024/genomes_50_rvs_reads.fastq -o spades_50_genome_contigs -t 12 --phred-offset 64
# spades.py --trusted-contigs /home/jyeh/summer2024/assembler_dict_scaffold_200_genomes.fasta -1 /home/jyeh/summer2024/genomes_200_fwd_reads.fastq -2 /home/jyeh/summer2024/genomes_200_rvs_reads.fastq -o spades_200_genome_contigs -t 12 --phred-offset 64

'''to run megahit'''
# megahit -1 ecoli_fwd_reads.fastq -2 ecoli_rvs_reads.fastq -o megahit_output -t 16
# -t = number of threads or cpus
# megahit -1 /u/c/yehjose1/summer2024/optimized_pcr_fwd_reads_16cycles.fastq -2 /u/c/yehjose1/summer2024/optimized_pcr_rvs_reads_16cycles.fastq -o megahit_output -t 16
# megahit -r /u/c/yehjose1/summer2024/SRA_reads/test_SRA.fastq -o /u/c/yehjose1/summer2024/SRA_reads/megahit_output -t 16



'''spades & megahit tgt'''
# spades.py --trusted-contigs /u/c/yehjose1/summer2024/megahit_output/final.contigs.fa -1 ecoli_fwd_reads.fastq -2 ecoli_rvs_reads.fastq --phred-offset 64 -o spades_output -t 16
#
# 

'''idba'''
# idba_ud --pre_correction --num_threads 16 -r ecoli_fwd_reads.fastq -r ecoli_rvs_reads.fastq -o idba_output

'''bash pileup'''
# samtools mpileup -f ecoli.fasta aligned_reads_sorted.bam > coverage.pileup
